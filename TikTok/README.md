# Overview 

The goal of this project is to build a classification model to help identify claims and opinions. Videos that are labeled opinions will be less likely to go on to be reviewed by a human moderator. Videos that are labeled as claims will be further sorted by a downstream process to determine whether they should get prioritized for review. For example, perhaps videos that are classified as claims would then be ranked by how many times they were reported, then the top x% would be reviewed by a human each day.

# Data Understanding

The data consisted of approximately 20k user submissions and 11 features. The features are listed below: 

| #  | Column                   | Non-Null Count | Dtype    |
|----|--------------------------|----------------|----------|
| 0  | claim_status             | 19084 non-null | object   |
| 1  | video_id                 | 19382 non-null | int64    |
| 2  | video_duration_sec       | 19382 non-null | int64    |
| 3  | video_transcription_text | 19084 non-null | object   |
| 4  | verified_status          | 19382 non-null | object   |
| 5  | author_ban_status        | 19382 non-null | object   |
| 6  | video_view_count         | 19084 non-null | float64  |
| 7  | video_like_count         | 19084 non-null | float64  |
| 8  | video_share_count        | 19084 non-null | float64  |
| 9  | video_download_count     | 19084 non-null | float64  |
| 10 | video_comment_count      | 19084 non-null | float64  |


Another feature was engineered to represent different information as shown below:
- `video_transcription_text_length` to capture the length for each transcription text

Below is a visualization of the distribution of `video_transcription_text` length for videos posted by: 
1. Verified accounts 
2. Unverified accounts

![Visualize the distribution of `video_transcription_text`](/images/0.png)

- The `video_transcription_text` length distributions for both claims and opinions are approximately normal with a slight right skew. Claim videos tend to have more characters&mdash;about 13 more on average

# Modeling and Evaluation 
- The confusion matrix below visualizes the results of the **`random forest model`**:
![Random Forest Evaluation](/images/1.png)
The confusion matrix indicates that there were 10 misclassifications&mdash;five false postives and five false negatives.


- The confusion matrix below visualizes the results of the **`XGBoost model`**:
![Random Forest Evaluation](/images/2.png)
The results of the **XGBoost model** were also nearly perfect. However, its errors tended to be false negatives. Identifying claims was the priority, so it's important that the model be good at capturing all actual claim videos. **The random forest model has a better `recall score`**, and is therefore the choosen model.

- The following plot displays the feature importances of random forest model:
![Random Forest Evaluation](/images/3.png)

**Insight:**
- The most predictive features all were related to engagement levels generated by the video.


# Conclusion

1. It is recommended to use this model because it performed well on both the validation and test holdout data. Furthermore, both precision and F<sub>1</sub> scores were consistently high. The model very successfully classified claims and opinions.
</br>
2. The model's most predictive features were all related to the user engagement levels associated with each video. It is classifying videos based on how many views, likes, shares, and downloads they received.
</br>
3. Because the model currently performs nearly perfectly, there is no need to engineer any new features.
</br>
4. To improve the performance of your model, the current version of the model does not need any new features. However, it would be helpful to have the number of times the video was reported. It would also be useful to have the total number of user reports for all videos posted by each author.
